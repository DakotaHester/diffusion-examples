{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056a146a",
   "metadata": {},
   "source": [
    "code adapted from https://nn.labml.ai/diffusion/ddpm/unet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96495476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dh2306/diffusion-examples/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import cv2 as cv\n",
    "from glob import glob\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers.models import UNet2DModel\n",
    "from diffusers.schedulers import DDIMScheduler\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import functional as tvf\n",
    "# from torchvision.transforms import \n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from torch import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "from typing import Optional, Union, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afb1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonize ps to s2 tiles\n",
    "s2_image_paths = glob('../dakota_sample_training_sr_images/*/*/s2_patch_*.png')\n",
    "\n",
    "if False:\n",
    "    for fp in tqdm(s2_image_paths):\n",
    "        \n",
    "        s2_img= cv.imread(fp)\n",
    "        s2_img = cv.cvtColor(s2_img, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        ps_fp = fp.replace('s2_patch_', 'ps_patch_')\n",
    "        ps_img = cv.imread(ps_fp)\n",
    "        ps_img = cv.cvtColor(ps_img, cv.COLOR_BGR2RGB)\n",
    "        ps_img_downsampled = cv.resize(ps_img, s2_img.shape[:2], interpolation=cv.INTER_LINEAR)\n",
    "        \n",
    "        ols = LinearRegression()\n",
    "        ols.fit(ps_img_downsampled.reshape(-1, 3), s2_img.reshape(-1, 3))\n",
    "        ps_img_harmonized = ps_img.reshape(-1, 3) @ ols.coef_.T + ols.intercept_\n",
    "        ps_img_harmonized = ps_img_harmonized.reshape(ps_img.shape)\n",
    "        \n",
    "        ps_img_harmonized = ps_img_harmonized.clip(0, 255)\n",
    "        ps_img_harmonized = ps_img_harmonized.astype(np.uint8)\n",
    "        ps_img_harmonized = cv.cvtColor(ps_img_harmonized, cv.COLOR_RGB2BGR)\n",
    "        cv.imwrite(ps_fp.replace('.png', '_harmonized.png'), ps_img_harmonized)\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e42c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class StandardDataAugmentations:\n",
    "    '''\n",
    "    Simple data augmentation that applies random rotation, horizontal, and vertical flips.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def __call__(X: torch.Tensor, y: Optional[torch.Tensor] = None):\n",
    "        \n",
    "        # do not resize this time, just apply random flip and color distortions\n",
    "        if torch.rand(1) > 0.5:\n",
    "            X = tvf.hflip(X)\n",
    "            if y is not None:\n",
    "                y = tvf.hflip(y)\n",
    "        \n",
    "        if torch.rand(1) > 0.5:\n",
    "            X = tvf.vflip(X)\n",
    "            if y is not None:\n",
    "                y = tvf.vflip(y)\n",
    "        \n",
    "        rot_angle = torch.randint(0, 4, (1,)).item()\n",
    "        X = tvf.rotate(X, rot_angle * 90)\n",
    "        if y is not None:\n",
    "            y = tvf.rotate(y, rot_angle * 90)\n",
    "    \n",
    "        \n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X\n",
    "    \n",
    "class PlanetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, s2_filepaths: Union[List[str], Tuple[str]], ps_filepaths: Union[List[str], Tuple[str]], transforms: Optional[StandardDataAugmentations] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.s2_filepaths = s2_filepaths\n",
    "        self.ps_filepaths = ps_filepaths\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.s2_filepaths)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _scale(\n",
    "        data, \n",
    "        in_range: Union[Tuple[int, int], Tuple[float, float]]=(0, 255), \n",
    "        out_range: Union[Tuple[int, int], Tuple[float, float]]=(-1.0, 1.0)\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # scale to 0-1\n",
    "        data = (data - in_range[0]) / (in_range[1] - in_range[0])\n",
    "        \n",
    "        # scale to out_range\n",
    "        data = data * (out_range[1] - out_range[0]) + out_range[0]\n",
    "        data = data.clamp(min=out_range[0], max=out_range[1])\n",
    "        return data\n",
    "    \n",
    "    def get_s2_img(self, idx):\n",
    "        \n",
    "        s2_img = cv.imread(self.s2_filepaths[idx])\n",
    "        s2_img = cv.cvtColor(s2_img, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        s2_img = torch.as_tensor(s2_img, dtype=torch.float32)\n",
    "        s2_img = s2_img.permute(2, 0, 1)\n",
    "        return self._scale(s2_img)\n",
    "    \n",
    "    def get_ps_img(self, idx, harmonize: bool=False, return_s2_img: bool=False):\n",
    "        \n",
    "        if return_s2_img and not harmonize:\n",
    "            raise ValueError('Cannot return Sentinel-2 image when harmonize is set to False')\n",
    "        \n",
    "        ps_img = cv.imread(self.ps_filepaths[idx])\n",
    "        ps_img = cv.cvtColor(ps_img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        ps_img = torch.as_tensor(ps_img, dtype=torch.float32)\n",
    "        ps_img = self._scale(ps_img)\n",
    "        ps_img = ps_img.permute(2, 0, 1)\n",
    "        return ps_img\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return self.get_ps_img(idx, harmonize=True, return_s2_img=True)\n",
    "        s2_img, ps_img = self.get_s2_img(idx), self.get_ps_img(idx)\n",
    "        if self.transforms is not None:\n",
    "            return self.transforms(s2_img, ps_img)\n",
    "        else:\n",
    "            return s2_img, ps_img\n",
    "\n",
    "\n",
    "# s2_image_paths = glob('/Volumes/dhester_ssd/dakota_sample_training_sr_images/*/*/s2_patch_*.png')\n",
    "# ps_image_paths = glob('/Volumes/dhester_ssd/dakota_sample_training_sr_images/*/*/ps_patch_*.png')\n",
    "s2_image_paths = glob('../dakota_sample_training_sr_images/*/*/s2_patch_*.png')\n",
    "ps_image_paths = [fp.replace('s2_patch_', 'ps_patch_').replace('.png', '_harmonized.png') for fp in s2_image_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5066a8fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) :-1: error: (-5:Bad argument) in function 'imread'\n> Overload resolution failed:\n>  - Expected 'filename' to be a str or path-like object\n>  - Expected 'filename' to be a str or path-like object\n>  - Expected 'filename' to be a str or path-like object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mPlanetDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# return self.get_ps_img(idx, harmonize=True, return_s2_img=True)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     s2_img, ps_img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_s2_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.get_ps_img(idx)\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms(s2_img, ps_img)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mPlanetDataset.get_s2_img\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_s2_img\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     s2_img = \u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ms2_filepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     s2_img = cv.cvtColor(s2_img, cv.COLOR_BGR2RGB)\n\u001b[32m     61\u001b[39m     s2_img = torch.as_tensor(s2_img, dtype=torch.float32)\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.12.0) :-1: error: (-5:Bad argument) in function 'imread'\n> Overload resolution failed:\n>  - Expected 'filename' to be a str or path-like object\n>  - Expected 'filename' to be a str or path-like object\n>  - Expected 'filename' to be a str or path-like object\n"
     ]
    }
   ],
   "source": [
    "dataset[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475a15e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13QGB 5712\n",
      "17MNQ 5438\n",
      "15SXS 1770\n",
      "10SGF 3935\n",
      "11TQH 6314\n",
      "16TFP 2919\n",
      "18TUL 4053\n",
      "17SKR 4837\n",
      "15TWH 967\n",
      "13QGF 2490\n",
      "['17MNQ'] ['13QGB', '15SXS', '10SGF', '11TQH', '16TFP', '18TUL', '17SKR', '15TWH', '13QGF']\n"
     ]
    }
   ],
   "source": [
    "random.seed(1701)\n",
    "\n",
    "unique_locations = set(path.split(os.sep)[-3] for path in s2_image_paths)\n",
    "for unique_location in unique_locations:\n",
    "    print(unique_location, len([path for path in s2_image_paths if path.split(os.sep)[-3] == unique_location]))\n",
    "\n",
    "val_sites = random.sample(sorted(unique_locations), k=1)\n",
    "train_sites = [site for site in unique_locations if site not in val_sites]\n",
    "\n",
    "print(val_sites, train_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9bc859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training dataset: 32997\n",
      "Number of samples in validation dataset: 5438\n"
     ]
    }
   ],
   "source": [
    "train_s2_paths = [fp for fp in s2_image_paths if fp.split(os.sep)[-3] in train_sites]\n",
    "train_ps_paths = [fp for fp in ps_image_paths if fp.split(os.sep)[-3] in train_sites]\n",
    "train_dataset = PlanetDataset(train_s2_paths, train_ps_paths, transforms=StandardDataAugmentations())\n",
    "print(f'Number of samples in training dataset: {len(train_dataset)}')\n",
    "\n",
    "val_s2_paths = [fp for fp in s2_image_paths if fp.split(os.sep)[-3] in val_sites]\n",
    "val_ps_paths = [fp for fp in ps_image_paths if fp.split(os.sep)[-3] in val_sites]\n",
    "val_dataset = PlanetDataset(val_s2_paths, val_ps_paths)\n",
    "print(f'Number of samples in validation dataset: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd663408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 14158147\n"
     ]
    }
   ],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=256,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    block_out_channels=[32, 64, 128, 256],\n",
    "    down_block_types=[\"DownBlock2D\"] * 4, # + ['AttnDownBlock2D'],\n",
    "    up_block_types=[\"UpBlock2D\"] * 4, # + ['AttnUpBlock2D']\n",
    ")\n",
    "\n",
    "print(f'Total parameters: {sum([p.numel() for p in model.parameters()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using backed {device}')\n",
    "if device.type == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    print('bfloat16 is supported. Using for mixed precision.')\n",
    "    mixed_precision_dtype = torch.bfloat16\n",
    "else:\n",
    "    print('bfloat16 not supported. Falling back to float16 for mixed precision.')\n",
    "    mixed_precision_dtype = torch.float16\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "batch_size = 256\n",
    "micro_batch_size = 64\n",
    "n_epochs = 100\n",
    "total_timesteps = 50\n",
    "warmup_epochs = 10\n",
    "\n",
    "grad_accum_steps = batch_size // micro_batch_size\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), fused=torch.cuda.is_available())\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs-1)\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - warmup_epochs)\n",
    "lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=total_timesteps)\n",
    "scaler = GradScaler()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=micro_batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=16)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=4)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    epoch_losses = []\n",
    "    with tqdm(train_dataloader, desc=f'Epoch {epoch}/{n_epochs}', unit='batch', postfix={'lr': optimizer.param_groups[0]['lr']}) as pbar:\n",
    "        for i, (_, X) in enumerate(train_dataloader):\n",
    "            \n",
    "            X = X.to(device)\n",
    "            noise = torch.randn(X.shape).to(device)\n",
    "            timesteps = torch.randint(0, total_timesteps, (micro_batch_size,)).to(device)\n",
    "            noisy_X = noise_scheduler.add_noise(X, noise, timesteps)\n",
    "\n",
    "            with autocast(device.type, dtype=mixed_precision_dtype):\n",
    "                noise_pred = model(noisy_X, timesteps).sample\n",
    "                loss = F.mse_loss(noise, noise_pred)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            pbar.set_postfix(lr=optimizer.param_groups[0]['lr'], loss=sum(epoch_losses) / len(epoch_losses))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    torch.save(model.state_dict(), '../models/ddim_planetscope.pt')\n",
    "    losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e544e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnoise_scheduler\u001b[49m.set_timesteps(num_inference_steps=\u001b[32m100\u001b[39m)\n\u001b[32m      3\u001b[39m x = torch.randn(\u001b[32m32\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m).to(device)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(noise_scheduler.timesteps)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1481\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1523\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1324\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1961\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/diffusion-examples/.venv/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/diffusion-examples/.venv/lib/python3.13/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "noise_scheduler.set_timesteps(num_inference_steps=100)\n",
    "\n",
    "x = torch.randn(32, 3, 256, 256).to(device)\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "    model_input = noise_scheduler.scale_model_input(x, t)\n",
    "    with torch.no_grad(), autocast(device.type, dtype=mixed_precision_dtype):\n",
    "        noise_pred = model(model_input, t).sample\n",
    "    x = noise_scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(x):\n",
    "        img = x[i]\n",
    "        img_display = (img.clamp(-1, 1) + 1) / 2\n",
    "        img_display = img_display.permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(img_display)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
