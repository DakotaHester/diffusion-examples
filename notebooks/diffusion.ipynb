{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056a146a",
   "metadata": {},
   "source": [
    "code adapted from https://nn.labml.ai/diffusion/ddpm/unet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96495476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dh2306/diffusion-examples/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import cv2 as cv\n",
    "from glob import glob\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers.models import UNet2DModel\n",
    "from diffusers.schedulers import DDIMScheduler\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import functional as tvf\n",
    "# from torchvision.transforms import \n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from torch import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "from typing import Optional, Union, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afb1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonize ps to s2 tiles\n",
    "s2_image_paths = glob('../dakota_sample_training_sr_images/*/*/s2_patch_*.png')\n",
    "\n",
    "if False:\n",
    "    for fp in tqdm(s2_image_paths):\n",
    "        \n",
    "        s2_img= cv.imread(fp)\n",
    "        s2_img = cv.cvtColor(s2_img, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        ps_fp = fp.replace('s2_patch_', 'ps_patch_')\n",
    "        ps_img = cv.imread(ps_fp)\n",
    "        ps_img = cv.cvtColor(ps_img, cv.COLOR_BGR2RGB)\n",
    "        ps_img_downsampled = cv.resize(ps_img, s2_img.shape[:2], interpolation=cv.INTER_LINEAR)\n",
    "        \n",
    "        ols = LinearRegression()\n",
    "        ols.fit(ps_img_downsampled.reshape(-1, 3), s2_img.reshape(-1, 3))\n",
    "        ps_img_harmonized = ps_img.reshape(-1, 3) @ ols.coef_.T + ols.intercept_\n",
    "        ps_img_harmonized = ps_img_harmonized.reshape(ps_img.shape)\n",
    "        \n",
    "        ps_img_harmonized = ps_img_harmonized.clip(0, 255)\n",
    "        ps_img_harmonized = ps_img_harmonized.astype(np.uint8)\n",
    "        ps_img_harmonized = cv.cvtColor(ps_img_harmonized, cv.COLOR_RGB2BGR)\n",
    "        cv.imwrite(ps_fp.replace('.png', '_harmonized.png'), ps_img_harmonized)\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e42c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class StandardDataAugmentations:\n",
    "    '''\n",
    "    Simple data augmentation that applies random rotation, horizontal, and vertical flips.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def __call__(X: torch.Tensor, y: Optional[torch.Tensor] = None):\n",
    "        \n",
    "        # do not resize this time, just apply random flip and color distortions\n",
    "        if torch.rand(1) > 0.5:\n",
    "            X = tvf.hflip(X)\n",
    "            if y is not None:\n",
    "                y = tvf.hflip(y)\n",
    "        \n",
    "        if torch.rand(1) > 0.5:\n",
    "            X = tvf.vflip(X)\n",
    "            if y is not None:\n",
    "                y = tvf.vflip(y)\n",
    "        \n",
    "        rot_angle = torch.randint(0, 4, (1,)).item()\n",
    "        X = tvf.rotate(X, rot_angle * 90)\n",
    "        if y is not None:\n",
    "            y = tvf.rotate(y, rot_angle * 90)\n",
    "    \n",
    "        \n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X\n",
    "    \n",
    "class PlanetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, s2_filepaths: Union[List[str], Tuple[str]], ps_filepaths: Union[List[str], Tuple[str]], transforms: Optional[StandardDataAugmentations] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.s2_filepaths = s2_filepaths\n",
    "        self.ps_filepaths = ps_filepaths\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.s2_filepaths)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _scale(\n",
    "        data, \n",
    "        in_range: Union[Tuple[int, int], Tuple[float, float]]=(0, 255), \n",
    "        out_range: Union[Tuple[int, int], Tuple[float, float]]=(-1.0, 1.0)\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # scale to 0-1\n",
    "        data = (data - in_range[0]) / (in_range[1] - in_range[0])\n",
    "        \n",
    "        # scale to out_range\n",
    "        data = data * (out_range[1] - out_range[0]) - out_range[1]\n",
    "        data = data.clamp(min=out_range[0], max=out_range[1])\n",
    "        return data\n",
    "    \n",
    "    def get_s2_img(self, idx):\n",
    "        \n",
    "        s2_img = cv.imread(self.s2_filepaths[idx])\n",
    "        s2_img = cv.cvtColor(s2_img, cv.COLOR_BGR2RGB)\n",
    "        s2_img = torch.as_tensor(s2_img, dtype=torch.float32)\n",
    "        s2_img = s2_img.permute(2, 0, 1)\n",
    "        return self._scale(s2_img)\n",
    "    \n",
    "    def get_ps_img(self, idx, harmonize: bool=False, return_s2_img: bool=False):\n",
    "        \n",
    "        if return_s2_img and not harmonize:\n",
    "            raise ValueError('Cannot return Sentinel-2 image when harmonize is set to False')\n",
    "        \n",
    "        ps_img = cv.imread(self.ps_filepaths[idx])\n",
    "        ps_img = cv.cvtColor(ps_img, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        if harmonize:\n",
    "            \n",
    "            s2_file = self.ps_filepaths[idx].replace('ps_patch_', 's2_patch_').replace\n",
    "            s2_img = cv.imread(s2_file)\n",
    "            s2_img = cv.cvtColor(s2_img, cv.COLOR_BGR2RGB)\n",
    "            \n",
    "            ps_img_downsampled = cv.resize(ps_img, s2_img.shape[:2], interpolation=cv.INTER_LINEAR)\n",
    "            \n",
    "            ols = LinearRegression()\n",
    "            ols.fit(ps_img_downsampled.reshape(-1, 3), s2_img.reshape(-1, 3))\n",
    "            ps_img_harmonized = ps_img.reshape(-1, 3) @ ols.coef_.T + ols.intercept_\n",
    "            # print(ols.coef_.T, ols.intercept_)\n",
    "            ps_img = ps_img_harmonized.reshape(ps_img.shape)\n",
    "\n",
    "        ps_img = torch.as_tensor(ps_img, dtype=torch.float32)\n",
    "        ps_img = self._scale(ps_img)\n",
    "        ps_img = ps_img.permute(2, 0, 1)\n",
    "        \n",
    "        if return_s2_img:\n",
    "            s2_img = torch.as_tensor(s2_img, dtype=torch.float32)\n",
    "            s2_img = self._scale(s2_img)\n",
    "            s2_img = s2_img.permute(2, 0, 1)\n",
    "            return ps_img, s2_img\n",
    "\n",
    "        else:\n",
    "            return ps_img\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return self.get_ps_img(idx, harmonize=True, return_s2_img=True)\n",
    "        s2_img, ps_img = self.get_s2_img(idx), self.get_ps_img(idx)\n",
    "        if self.transforms is not None:\n",
    "            return self.transforms(s2_img, ps_img)\n",
    "        else:\n",
    "            return s2_img, ps_img\n",
    "\n",
    "\n",
    "# s2_image_paths = glob('/Volumes/dhester_ssd/dakota_sample_training_sr_images/*/*/s2_patch_*.png')\n",
    "# ps_image_paths = glob('/Volumes/dhester_ssd/dakota_sample_training_sr_images/*/*/ps_patch_*.png')\n",
    "s2_image_paths = glob('../dakota_sample_training_sr_images/*/*/s2_patch_*.png')\n",
    "ps_image_paths = [fp.replace('s2_patch_', 'ps_patch_').replace('.png', '_harmonized.png') for fp in s2_image_paths]\n",
    "print(os.path.exists(ps_image_paths[0]))\n",
    "# ps_image_paths = glob('../dakota_sample_training_sr_images/*/*/ps_patch_*_harmonized.png')\n",
    "\n",
    "dataset = PlanetDataset(\n",
    "    s2_filepaths=s2_image_paths,\n",
    "    ps_filepaths=ps_image_paths\n",
    ")\n",
    "\n",
    "# n_idx = 20\n",
    "# fig, ax = plt.subplots(n_idx, 3, figsize=(6, 2*n_idx))\n",
    "\n",
    "# for i in range(n_idx):\n",
    "#     harm_ps_img, s2_img = dataset.get_ps_img(i+10, harmonize=True, return_s2_img=True)\n",
    "#     ps_img = dataset.get_ps_img(i+10, harmonize=False)\n",
    "\n",
    "#     ax[i][0].imshow((ps_img.permute(1, 2, 0) + 1) / 2)\n",
    "#     ax[i][1].imshow((harm_ps_img.permute(1, 2, 0) + 1) / 2)\n",
    "#     ax[i][2].imshow((s2_img.permute(1, 2, 0) + 1) / 2)\n",
    "\n",
    "# for axis in ax.ravel():\n",
    "#     axis.axis('off')\n",
    "    \n",
    "# ax[0][0].set_title('Original PS ortho')\n",
    "# ax[0][1].set_title('Harmonized PS ortho')\n",
    "# ax[0][2].set_title('Sentinel-2 ortho')\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475a15e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13QGB 5712\n",
      "17MNQ 5438\n",
      "15SXS 1770\n",
      "10SGF 3935\n",
      "11TQH 6314\n",
      "16TFP 2919\n",
      "18TUL 4053\n",
      "17SKR 4837\n",
      "15TWH 967\n",
      "13QGF 2490\n",
      "['17MNQ'] ['13QGB', '15SXS', '10SGF', '11TQH', '16TFP', '18TUL', '17SKR', '15TWH', '13QGF']\n"
     ]
    }
   ],
   "source": [
    "random.seed(1701)\n",
    "\n",
    "unique_locations = set(path.split(os.sep)[-3] for path in s2_image_paths)\n",
    "for unique_location in unique_locations:\n",
    "    print(unique_location, len([path for path in s2_image_paths if path.split(os.sep)[-3] == unique_location]))\n",
    "\n",
    "val_sites = random.sample(sorted(unique_locations), k=1)\n",
    "train_sites = [site for site in unique_locations if site not in val_sites]\n",
    "\n",
    "print(val_sites, train_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9bc859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training dataset: 32997\n",
      "Number of samples in validation dataset: 5438\n"
     ]
    }
   ],
   "source": [
    "train_s2_paths = [fp for fp in s2_image_paths if fp.split(os.sep)[-3] in train_sites]\n",
    "train_ps_paths = [fp for fp in ps_image_paths if fp.split(os.sep)[-3] in train_sites]\n",
    "train_dataset = PlanetDataset(train_s2_paths, train_ps_paths, transforms=StandardDataAugmentations())\n",
    "print(f'Number of samples in training dataset: {len(train_dataset)}')\n",
    "\n",
    "val_s2_paths = [fp for fp in s2_image_paths if fp.split(os.sep)[-3] in val_sites]\n",
    "val_ps_paths = [fp for fp in ps_image_paths if fp.split(os.sep)[-3] in val_sites]\n",
    "val_dataset = PlanetDataset(val_s2_paths, val_ps_paths)\n",
    "print(f'Number of samples in validation dataset: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd663408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 14158147\n"
     ]
    }
   ],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=256,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    block_out_channels=[32, 64, 128, 256],\n",
    "    down_block_types=[\"DownBlock2D\"] * 4, # + ['AttnDownBlock2D'],\n",
    "    up_block_types=[\"UpBlock2D\"] * 4, # + ['AttnUpBlock2D']\n",
    ")\n",
    "\n",
    "print(f'Total parameters: {sum([p.numel() for p in model.parameters()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backed cuda\n",
      "bfloat16 is supported. Using for mixed precision.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 515/515 [06:19<00:00,  1.36batch/s, loss=0.505, lr=0.0001]\n",
      "Epoch 2/100: 100%|██████████| 515/515 [06:04<00:00,  1.41batch/s, loss=0.21, lr=0.0002] \n",
      "Epoch 3/100: 100%|██████████| 515/515 [05:58<00:00,  1.44batch/s, loss=0.173, lr=0.0003]\n",
      "Epoch 4/100: 100%|██████████| 515/515 [05:58<00:00,  1.44batch/s, loss=0.153, lr=0.0004]\n",
      "Epoch 5/100: 100%|██████████| 515/515 [05:58<00:00,  1.44batch/s, loss=0.144, lr=0.0005]\n",
      "Epoch 6/100: 100%|██████████| 515/515 [05:58<00:00,  1.44batch/s, loss=0.136, lr=0.0006]\n",
      "Epoch 7/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.132, lr=0.0007]\n",
      "Epoch 8/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.131, lr=0.0008]\n",
      "Epoch 9/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.128, lr=0.0009]\n",
      "Epoch 10/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.126, lr=0.001]\n",
      "/home/dh2306/diffusion-examples/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.124, lr=0.001]\n",
      "Epoch 12/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.123, lr=0.001]\n",
      "Epoch 13/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.124, lr=0.000999]\n",
      "Epoch 14/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.121, lr=0.000997]\n",
      "Epoch 15/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=0.74, lr=0.000995] \n",
      "Epoch 16/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=1, lr=0.000992]\n",
      "Epoch 17/100: 100%|██████████| 515/515 [05:57<00:00,  1.44batch/s, loss=1, lr=0.000989]\n",
      "Epoch 18/100: 100%|██████████| 515/515 [06:01<00:00,  1.42batch/s, loss=1, lr=0.000985]\n",
      "Epoch 19/100:  20%|██        | 103/515 [03:48<15:12,  2.21s/batch, loss=1, lr=0.000981]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     noise_pred = model(noisy_X, timesteps).sample\n\u001b[32m     46\u001b[39m     loss = F.mse_loss(noise, noise_pred)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m epoch_losses.append(loss.item())\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % grad_accum_steps == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/diffusion-examples/.venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/diffusion-examples/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/diffusion-examples/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using backed {device}')\n",
    "if device.type == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    print('bfloat16 is supported. Using for mixed precision.')\n",
    "    mixed_precision_dtype = torch.bfloat16\n",
    "else:\n",
    "    print('bfloat16 not supported. Falling back to float16 for mixed precision.')\n",
    "    mixed_precision_dtype = torch.float16\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "batch_size = 256\n",
    "micro_batch_size = 64\n",
    "n_epochs = 100\n",
    "total_timesteps = 50\n",
    "warmup_epochs = 10\n",
    "\n",
    "grad_accum_steps = batch_size // micro_batch_size\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), fused=torch.cuda.is_available())\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs-1)\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - warmup_epochs)\n",
    "lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=total_timesteps)\n",
    "scaler = GradScaler()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=micro_batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=16)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=4)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    epoch_losses = []\n",
    "    with tqdm(train_dataloader, desc=f'Epoch {epoch}/{n_epochs}', unit='batch', postfix={'lr': optimizer.param_groups[0]['lr']}) as pbar:\n",
    "        for i, (_, X) in enumerate(train_dataloader):\n",
    "            \n",
    "            X = X.to(device)\n",
    "            noise = torch.randn(X.shape).to(device)\n",
    "            timesteps = torch.randint(0, total_timesteps, (micro_batch_size,)).to(device)\n",
    "            noisy_X = noise_scheduler.add_noise(X, noise, timesteps)\n",
    "\n",
    "            with autocast(device.type, dtype=mixed_precision_dtype):\n",
    "                noise_pred = model(noisy_X, timesteps).sample\n",
    "                loss = F.mse_loss(noise, noise_pred)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            pbar.set_postfix(lr=optimizer.param_groups[0]['lr'], loss=sum(epoch_losses) / len(epoch_losses))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    torch.save(model.state_dict(), '../models/ddim_planetscope.pt')\n",
    "    losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e544e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler.set_timesteps(num_inference_steps=100)\n",
    "\n",
    "x = torch.randn(32, 3, 256, 256).to(device)\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "    model_input = noise_scheduler.scale_model_input(x, t)\n",
    "    with torch.no_grad(), autocast(device.type, dtype=mixed_precision_dtype):\n",
    "        noise_pred = model(model_input, t).sample\n",
    "    x = noise_scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(x):\n",
    "        img = x[i]\n",
    "        img_display = (img.clamp(-1, 1) + 1) / 2\n",
    "        img_display = img_display.permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(img_display)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
